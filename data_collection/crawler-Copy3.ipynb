{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import urllib.request\n",
    "import requests\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://memegenerator.net/memes/popular/alltime/page/\"\n",
    "\n",
    "columns = ['Title', 'Link', 'Image']\n",
    "data = DataFrame(columns=columns)\n",
    "\n",
    "for page in range(1, 201):\n",
    "    if page % 20 == 0:\n",
    "        print(page)\n",
    "    url = base_url + str(page)\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text)\n",
    "    img_divs = soup.findAll('div', {'class': 'char-img'})\n",
    "    for img_div in img_divs:\n",
    "        img_src = img_div.find('img').get('src')\n",
    "        link = img_div.find('a').get('href')\n",
    "        title = img_div.find('div', {'class': 'left'}).text.strip()\n",
    "        tmp = DataFrame([[title, link, img_src]])\n",
    "        tmp.columns = columns\n",
    "        data = data.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/meme_metadata.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/meme_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THESE NUMBERS DEPENDING ON THE MEMES YOU ARE SUPPOSED TO CRAWL\n",
    "start = 0 + 300\n",
    "end = start + 100\n",
    "data = data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_web_image(url, name):\n",
    "    r = requests.get(url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "    full_name = '../data/images' + str(name) + '.jpg'\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        with open(full_name, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    else:\n",
    "        print('Could not download ' + str(name))\n",
    "    return full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Title', 'Link', 'CaptionTop', 'CaptionBottom', 'ImageName', 'Upvotes']\n",
    "caption_data = DataFrame(columns=columns)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if index % 500 == 0:\n",
    "        print(index)\n",
    "    meme_link = row['Link']\n",
    "    img_link = row['Image']\n",
    "    img_name = row['Title']\n",
    "    download_name = download_web_image(img_link, meme_link)\n",
    "    base_url = 'https://memegenerator.net'\n",
    "    page = 1\n",
    "    flag = True\n",
    "    for page in range(1, 500):\n",
    "        url = base_url + row['Link'] + '/images/popular/alltime/page/' + str(page)\n",
    "        source_code = requests.get(url)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text)\n",
    "        for meme in soup.findAll('div', {'class': 'gallery-img'}):\n",
    "            caption = meme.find('div', {'class': 'optimized-instance-container'})\n",
    "            text0 = caption.find('div', {'class': 'optimized-instance-text0'}).text.strip()\n",
    "            text1 = caption.find('div', {'class': 'optimized-instance-text1'}).text.strip()\n",
    "            upvotes = int(meme.find('div', {'class': 'score'}).text.replace(',', '').strip())\n",
    "            if upvotes < 0:\n",
    "                continue\n",
    "            tmp = DataFrame([[img_name, meme_link, text0, text1, download_name, upvotes]])\n",
    "            tmp.columns = columns\n",
    "            caption_data = caption_data.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_data.to_csv('../data/caption_data_' + str(start) + '_' + str(end) + '.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
