{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/as3ek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from scipy.misc import imread, imresize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from random import randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/caption_data_0_100.csv') \n",
    "image_folder = 'data/images/' \n",
    "captions_per_image = 7000\n",
    "min_word_freq = 5\n",
    "output_folder = 'data/proc_data_files/'\n",
    "max_len = 20\n",
    "\n",
    "train_image_paths = []\n",
    "train_image_captions = []\n",
    "val_image_paths = []\n",
    "val_image_captions = []\n",
    "test_image_paths = []\n",
    "test_image_captions = []\n",
    "num_images_to_train = 20\n",
    "word_freq = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataset['ImageName'][:num_images_to_train]:\n",
    "    captions = []\n",
    "    for c in dataset[dataset['ImageName'] == img]['Caption']:\n",
    "        # Updating word freq\n",
    "        c = str(c)\n",
    "        \n",
    "        tokens = word_tokenize(c)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        word_freq.update(tokens)\n",
    "        if len(tokens) <= max_len:\n",
    "            captions.append(tokens)\n",
    "    \n",
    "    if len(captions) == 0:\n",
    "        continue\n",
    "    \n",
    "    path = os.path.join(img)\n",
    "    \n",
    "    if randint(0, 10) < 9:\n",
    "        train_image_paths.append(path)\n",
    "        train_image_captions.append(captions)\n",
    "    \n",
    "    else:\n",
    "        val_image_paths.append(path)\n",
    "        val_image_captions.append(captions)\n",
    "        \n",
    "    if randint(0, 10) < 5:\n",
    "        test_image_paths.append(path)\n",
    "        test_image_captions.append(captions)\n",
    "        \n",
    "# Sanity check\n",
    "assert len(train_image_paths) == len(train_image_captions)\n",
    "assert len(val_image_paths) == len(val_image_captions)\n",
    "assert len(test_image_paths) == len(test_image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word map\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base/root name for all output files\n",
    "base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
